# Simulator
This folder contains a wideband simulator that allows to perform experiments of the DANSE algorithm. The term `wideband` implies that actual speech signals can be processed and that the processing works in the frequency domain by means of a WOLA processing chain. 

It is important to note that it is assumed all nodes have the same number of sensors (= microphones) and that they all share the same base sampling frequency. (An option is incorporated to enable a fixed, non time-varying, sampling rate offset is included. However, sampling rate offset estimation and/or compensation has not been included.)

- [Folder structure](#folder-structure)
- [Configuration](#configuration-of-the-experiments)
- [Output](#output)
- [Known issues & limitations](#known-issues--limitations)

## Folder structure
There are 5 main entry points to this code:

1. [main_batch.py](./main_batch.py): performs so-called batch-mode simulations where the complete signal is used in every iteration. Output: generates a set of figures (not saved) and some audio files stored in `output/audio`.
2. [main_online.py](./main_online.py): performs online-mode simulations where a different signal segment is used at every iteration. Same output as [main_batch.py](./main_batch.py).
3. [main_sweep.py](./main_sweep.py): performs a parameter sweep for online-mode experiments. Which parameters are swept over is controlled by an auxiliary file called `sweep_variables.py` of which an example is given in [sweep_variables_example](./sweep_variables_example.py). This file generates a `.csv` file that can afterwards be postprocessed by the [visualize_sweeps.py](./visualize_sweeps.py). Results stored in `output/sweeps/name_of_sweep`.
4. [visualize_sweeps.py](./visualize_sweeps.py): takes in the name of a folder that contains the `.csv` generated by [main_sweep.py](./main_sweep.py) and postprocesses it. Also controlled by an auxiliary file `visualization_variables.py` of which an example is given in [visualization_variables_example.py](./visualization_variables_example.py).
5. [preprocess_raw_audio.ipynb](./preprocess_raw_audio.ipynb): notebook that allows to preprocess two speech corpora that were used during the development of this toolbox; [LibriSpeech](https://www.openslr.org/12) (often considered to be less "clean") and [VCTK](https://datashare.ed.ac.uk/handle/10283/3443) (more clean, but quite large). The preprocessing mainly boils down to concatenating all segments of the same speaker/story. It also includes the option to interleave silences between sentences.
> [!IMPORTANT]
> When preprocessing the data and operation using a VAD is desired, it is recommended to interleave at least some amount of silence between sentences to allow for proper estimation of $\mathbf{R}_{nn}$ as it can only be estimated during segments where the desired signal is inactive.

> [!IMPORTANT]
> To obtain decent performance, the noise should be at least short-term stationary. Therefore it is recommended not to use plain speech signals but rather white or babble noise.

> [!NOTE]
> Running this notebook might require also installing the `ipykernel` pip package to run it in something like VSCode. Alternatively it can also be run in something like `JupyterLab`. Both are installable through `pip`. However, neither are present in the requirements files and hence are not installed by default.

> [!CAUTION]
> To obtain a speedup of the experiments, it is often opted to collect a set of signals prior to performing the ISTFT as this allows the ISTFT to be computed in a multi-threaded fashion. However, this leads to quite high memory usage. ($\pm$ 10 GB for running `main_online.py` with $lFFT = 1024$ and a duration of 40s.)

## Configuration of the experiments
Configuration of the three `main_x.py` files happens through a configuration file in the `YAML` format. An example of this file is given in [cfg_example.yml](./config/cfg_example.yml). It should be noted however that the scripts overwrite some of these settings: 
- [main_batch.py](./main_batch.py) and [main_online.py](./main_online.py) have a brief configuration section in the beginning of the script that allows to determine which node to track alongside which exact algorithms to study in this specific setup. This means that the `GEVD` and `sequential` tags in the config are overwritten in these scripts! Furthermore, they also allow to control which algorithms to make more expensive plots of and write back the audio output.
- [main_sweep.py](./main_sweep.py) always loads in the entire config file, but then makes adjustments which are determined based on the `sweep_variables.py` file which determines what parameter combinations to sweep over. Base your `sweep_variables.py` file on the example given in [sweep_variables_example.py](./sweep_variables_example.py). It should be noted that coupling variables is also possible (for example, sweeping over the $\Gamma$ hyperparameter for GEVD-based algorithms is not really sensible as $L_2$-regularization is not applied to it.) 

  The config file is also stored in the same folder as the output csv of a sweep such that you can know under which exact circumstances the sweep happened.

> [!NOTE]
> For convenience the config files `cfg.yml`, `sweep_variables.py` and `visualization_variables.py` are included in the `.gitignore` to prevent pushing config files which makes the commits more verbose.

> [!CAUTION]
> Some options are incorporated for experimentation, but might actually harm performance. For example, using the `include_silences` flag does enforce silences in the desired signal such that $\mathbf{R}_\mathbf{nn}$ can be computed without oracle knowledge of the separate contributions. However, doing so can cut off the waveform, leading to the VAD not recognizing it as a speech segment, which leads to the desired signal leaking into $\mathbf{R}_\mathbf{nn}$. This in turn leads to reduced performance as the desired signal will also be supressed. When white noise is used with an energy-based VAD this option might be useful however.

## Output
The output can consist of a few things, as mentioned before:
- Figures: not saved programmatically, should be done manually if desired. 
- Audio files: only for the algorithms under `desiredAlgos` in the `main_batch.py` and `main_online.py` files. The output is stored under `/path/to/current-working-directory/output/audio`. 
- Data files: for the output of sweeps. The output is stored under `/path/to/current-working-directory/output/sweeps/unique-id`. The `unique-id` folder is automatically created.

To prevent crashes, ensure the `output/audio` and `output/sweeps` folders exist and that all paths match:
- If the scripts are ran from the root directory, create `output/audio` and `output/sweeps` and set the `PATH_TO_CFG` variable to `simulator/config/cfg.yml`. 
- If they are ran from the simulator directory, create `simulator/output/audio` and `simulator/output/sweeps` and set `PATH_TO_CFG` to `config/cfg.yml`.

## Known issues & limitations
1. There is a memory leak in the parameter sweeps, presumably due to a self-reference in scipy's `ShortTimeFFT` object. This can lead to the memory consumption of the sweeps exceeding 30 GB of ram when performing a sweep with 256 combinations with 40 seconds of audio.
2. The `spatialResponse` class, plotting the response of the filter to speaker locations in the room only works for 2D scenario's. Additionally, the higher the reverberation time ($T_{60}$) of the room, the higher the computation time and memory consumption of this class and the invocations to plot. For example, for a $T_{60}$ of 0.5s, plotting 4 responses will again lead to the memory consumption exceeding 30 GB. Therefore, it is recommended to only turn this on when specifically desired.

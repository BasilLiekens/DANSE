{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess raw audio\n",
    "This notebook handles the preprocessing of speech corpora. \n",
    "\n",
    "The first corpus handled is the [LibriSpeech corpus](https://www.openslr.org/12), and more specifically the `dev-clean.tar.gz` version of it. The goal is to compose all individual fragments of one \"story\" into one big audio file that can be used later on for further processing.\n",
    "\n",
    "A second corpus that is handled is the [VCTK corpus](https://datashare.ed.ac.uk/handle/10283/3443), which can be considered to be more \"clean\" than `LibriSpeech`.\n",
    "\n",
    "Both corpora have dedicated cells that allow to process them, both a function and a calling cell.\n",
    "\n",
    "It should be noted that the `silencePeriod` should ideally be chosen different from 0 to account for the fact that in a true online-mode operation the noise-only segments are used to estimate $\\mathbf{R}_{\\mathbf{nn}}$. If the sentences are concatenated directl\n",
    "y, the estimate of this autocorrelation matrix will be very poor if it is ever estimated at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import signal\n",
    "import soundfile as sf\n",
    "\n",
    "rawAudioPath = os.path.join(\"path\", \"to\", \"raw\", \"audio\")\n",
    "processedPath = os.path.join(\"path\", \"to\", \"folder\", \"to\", \"store\", \"results\")\n",
    "\n",
    "expectedFs = 16000  # If sampling frequency not equal to this, resample\n",
    "silencePeriod = 0.5  # [s]: time between sentences to allow for estimating Rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleSessionLibriSpeech(\n",
    "    sessionPath: str,\n",
    "    processedPath: str,\n",
    "    subject: str,\n",
    "    session: str,\n",
    "    expectedFs: int,\n",
    "    silencePeriod: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the path to a directory containing all recordings of a certain\n",
    "    session, stitch all recordings of the session together to obtain one large\n",
    "    recording and save it.\n",
    "    \"\"\"\n",
    "    recordings = []\n",
    "    sessionData = []\n",
    "\n",
    "    for recording in os.scandir(sessionPath):\n",
    "        path = recording.path\n",
    "        # collect all the recordings, and remove the transcript (in .txt form)\n",
    "        if path.split(\".\")[-1] == \"flac\":\n",
    "            recordings.append(path)\n",
    "\n",
    "    # sort the list to have the right sequence of recordings\n",
    "    recordings.sort(key=lambda x: x.split(\".\")[-2].split(\"-\")[-1])\n",
    "\n",
    "    # read the recordings on at a time and stitch together\n",
    "    for recording in recordings:\n",
    "        data, fs = sf.read(recording)\n",
    "        if fs != expectedFs:  # resample\n",
    "            data = signal.resample_poly(data, up=expectedFs, down=fs)\n",
    "        if data.ndim > 1:\n",
    "            if data.shape[0] == 2:\n",
    "                data = data.T  # enforce data to be column vectors\n",
    "            if data.shape[1] == 2:\n",
    "                data = data[:, 0]  # convert stereo to mono\n",
    "\n",
    "        sessionData.append(data)\n",
    "        sessionData.append(np.zeros(int(expectedFs * silencePeriod)))\n",
    "\n",
    "    sessionData = np.concatenate((sessionData), axis=0)\n",
    "    sessionData = sessionData / np.max(np.abs(sessionData))\n",
    "\n",
    "    sf.write(\n",
    "        os.path.join(processedPath, subject, session + \".wav\"), sessionData, expectedFs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleSessionVCTK(\n",
    "    subjectPath: str,\n",
    "    saveFolder: str,\n",
    "    subjectID: str,\n",
    "    expectedFs: int,\n",
    "    silencePeriod: float,\n",
    "):\n",
    "    \"\"\"\n",
    "    Given the path to a directory containing all recordings of a certain\n",
    "    session, stitch all recordings of the session together to obtain one large\n",
    "    recording and save it.\n",
    "\n",
    "    Since VCTK has a \"mic_1\" and \"mic_2\" per recording, this is filtered to\n",
    "    avoid having nigh the same pronounciation twice.\n",
    "    \"\"\"\n",
    "    recordings = []\n",
    "    sessionData = []\n",
    "\n",
    "    for recording in os.scandir(subjectPath):  # no transcripts as in Librispeech\n",
    "        recordingName = os.path.split(recording.path)\n",
    "        if \"mic1\" in recordingName[1]:  # only retain mic 1\n",
    "            recordings.append(recording.path)\n",
    "\n",
    "    for recording in recordings:\n",
    "        data, fs = sf.read(recording)\n",
    "        if data.ndim != 1:  # get only one dimension\n",
    "            if data.shape[0] == 2:\n",
    "                data = data[0, :]\n",
    "            else:\n",
    "                data = data[:, 0]\n",
    "\n",
    "        if fs != expectedFs:  # resample if needed\n",
    "            data = signal.resample_poly(data, up=expectedFs, down=fs)\n",
    "\n",
    "        sessionData.append(data)\n",
    "        sessionData.append(np.zeros(int(silencePeriod * expectedFs)))\n",
    "\n",
    "    sessionData = np.concatenate(sessionData, axis=0)\n",
    "    sessionData = sessionData / (np.max(np.abs(sessionData)))\n",
    "\n",
    "    sf.write(os.path.join(saveFolder, subjectID + \".wav\"), sessionData, expectedFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all sessions for LibriSpeech\n",
    "for entry in os.scandir(rawAudioPath):\n",
    "    subjectPath = entry.path\n",
    "    subjectID = subjectPath.split(\"/\")[-1]\n",
    "    print(f\"Subject {subjectID} started\")\n",
    "\n",
    "    saveFolder = os.path.join(processedPath, subjectID)\n",
    "    if not os.path.isdir(saveFolder):\n",
    "        os.mkdir(saveFolder)\n",
    "\n",
    "    for session in os.scandir(subjectPath):\n",
    "        sessionPath = session.path\n",
    "        sessionID = sessionPath.split(\"/\")[-1]\n",
    "        handleSessionLibriSpeech(\n",
    "            session.path, saveFolder, subjectID, sessionID, expectedFs\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect all sessions for VCTK\n",
    "for entry in os.scandir(rawAudioPath):\n",
    "    if os.path.isdir(entry):\n",
    "        subjectPath = entry.path\n",
    "        subjectID = subjectPath.split(\"/\")[-1]\n",
    "        print(f\"Subject {subjectID} started\")\n",
    "\n",
    "        if not os.path.isdir(processedPath):\n",
    "            os.mkdir(processedPath)\n",
    "\n",
    "        handleSessionVCTK(\n",
    "            subjectPath,\n",
    "            processedPath,\n",
    "            subjectID,\n",
    "            expectedFs=expectedFs,\n",
    "            silencePeriod=silencePeriod,\n",
    "        )  # no sessions for VCTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
